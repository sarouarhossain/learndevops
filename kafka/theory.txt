Topics
-------
-> similar to database table
-> identified by name

Partitions
----------
-> Topics are split in Partitions

Kafka topic -> Partition 0
            -> Partition 1
            -> Partition 2

-> each partition is orderded 
-> Each message within the partition gets an incremental id called offset.
-> offset only have a meaning for specific partition.
-> Oeder is guranteed only within the partition (not across partitions)
-> data is kept only for limited time (default one week)
-> but offsets keep on incrementing 
-> once data is written to a partition, it can't be changed(immutability)
-> By default data will be written in partitions randomly

Cluster
-------
A kafka Cluster is composed of multiple Brokers

Brokers
-------
-> What holds the partitions ?? Ans: Brokers
-> is Broker basically a server
-> Each broker is identified by an ID (Integer)
-> Each broker contains certain topic partitions. Each broker has data, but not all data. 
-> After connecting to nay broker (called bootstrap broker), you will be connected to the entire Cluster.
-> 3 ~ 100 Brokers

Replication Factor
------------------
When machine goes down what will happen ?
-> here comes the replication Factor
-> topic should have a replication factor > 1 (usually 2 and 3)
-> 2 is risky, 3 is standard
-> this way if a broker is down, another broker can serve data

Leader for a partition
----------------------
-> At any time only one broker can be a leader for a given partition
-> Only that leadere can receive and serve data for a partition
-> Other broker will synchronize the data
-> each partition has one leader and multiple ISR(in-sync replica)
-> Zookeeper will deciode leader and in-sync replica
 
 Producers
 -----------
 -> Producers write data to topics
 -> Producers automatically know to which broker and partition to write
 -> In case of broker failures, Producers will automatically recover
 -> Producers can choose to receive acknowledgment of data writes.
        * acks = 0 (Producer won't wait for acknowledgment. possible data loss)
        * acks = 1 (default, Producer will wait for leader acknowledgment. limited data loss)
        * acks = all (leader + all replicaa acknowledgment. no data loss)

Producers: Message keys
-----------------------
-> Producers can choose to send a key with the message (key: string, number etc ..)
-> key = null, data is sent round robin (broker 101, then broker 102, then broker 103)
-> if a key is sent, all the messages for that key will always go to the same partition [using key hashing]
-> A key is basically sent, if you need message ordering for a specific field (ex; truck_id)

Consumers
---------
-> read data from a topic
-> Consumers know which broker to read from
-> In case of broker failures, consumers know how to recover
-> Data is read in order within each partitions

Consumer Group
--------------
-> Consumers read data in groups
-> Each consumer within a group reads from exclusive partitions
-> if you have more consumers than partitions, some consumers will be inactive
-> Consumers will automatically use a GroupCoordinator and a ConsumerCoordinator to assign a consumers to a partition.

Consumer offsets
-----------------
-> Kafka stores the offset at which a consumer group has been reading
-> The offsets committed live in a kafka topic named __consumer_offsets
-> when a consumer in a group has processed data received from kafka, it should be committing the offsets
-> If a consumer dies, it will be able to read back from where it left off, thanks to the committed consumer offsets.

Delivery semantics for Consumers
---------------------------------
-> consumers choose when to commit offsets
-> There are 3 delivery semantics
    * At most once: 
        => offsets are commited as soon as message is received
        => if the processing goes wrong, the message will be lost (it won't be read again)
    * At least once (usually preferred):
        => offsets are committed after the message is processed
        => If the procesing goes wrong the message will be read again
        => this can result duplicate processing of messages. make sure your processing is idempotent.
           Processing again the message won't impact your system. 
    * Exactly once:
        => can be achieved for Kafka => Kafka workflows using Kafka streams api
        => For Kafka => External System workflows, use an idempotent consumer.

Kafka Broker Discovery
---------------------
-> every kafka broker is also called a "bootstrap server"
-> that means that you only need to connect to one broker, and you will be connected to entire Cluster
-> each broker knows about all brokers, topics and partitions (metadata)

Zookeeper
-------
-> manages the brokers (keeps a list of them)
-> zookeeper helps in performing leader election for partitions
-> zookeeper sends the notifications to kafka in case of chnages (new topics, broker dies, broker comes up, delete topics etc..)
-> kafka can't work eithout zookeeper
-> zookeeper design operates with an odd number of servers (3,5, 7)
